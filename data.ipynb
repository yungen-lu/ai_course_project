{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_missing_rate(df: pd.DataFrame):\n",
    "    missing = pd.DataFrame(df.isnull().sum()).reset_index()\n",
    "    missing.columns = ['Feature', 'Missing_Percent(%)']\n",
    "    missing['Missing_Percent(%)'] = missing['Missing_Percent(%)'].apply(lambda x: x / df.shape[0] * 100)\n",
    "    print(missing.loc[missing['Missing_Percent(%)']>0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data = pd.read_csv('./data/US_Accidents_March23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [\n",
    "    'Source', 'Description', 'End_Lat', 'End_Lng',\n",
    "    'Zipcode', 'Timezone', 'Airport_Code', 'ID',\n",
    "    'Turning_Loop', 'Country', 'Precipitation(in)', 'Wind_Chill(F)'\n",
    "]\n",
    "ori_data.drop(drop_list,axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data.drop(ori_data[ori_data['Nautical_Twilight'].isnull()].index, inplace=True)\n",
    "ori_data.insert(loc=1, column='Twilight', value=[1]*len(ori_data))\n",
    "twilight_list = ['Sunrise_Sunset', 'Civil_Twilight','Nautical_Twilight', 'Astronomical_Twilight']\n",
    "#accumulate the twilight data\n",
    "for tl in twilight_list:\n",
    "    ori_data[tl] = ori_data[tl].apply(lambda x: 1 if x == 'Day' else 0)\n",
    "def set_day_or_night(x):\n",
    "    if x > 2:\n",
    "        return 1\n",
    "    elif x == 2:\n",
    "        if random.random() > 0.5:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "ori_data['Twilight'] = ori_data[twilight_list].sum(axis=1).apply(set_day_or_night)\n",
    "ori_data.drop(twilight_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data['Start_Time']=pd.to_datetime(ori_data['Start_Time'], format='ISO8601')\n",
    "ori_data['End_Time']=pd.to_datetime(ori_data['End_Time'], format='ISO8601')\n",
    "ori_data['elapsed_time'] = ori_data['End_Time'] - ori_data['Start_Time']\n",
    "ori_data['elapsed_time'] = ori_data['elapsed_time']/np.timedelta64(1,'m')\n",
    "\n",
    "ori_data['Hour'] = ori_data['Start_Time'].dt.hour\n",
    "ori_data['Minute']=ori_data['Hour']*60.0+ori_data[\"Start_Time\"].dt.minute\n",
    "ori_data['Month'] = ori_data['Start_Time'].dt.month\n",
    "\n",
    "nmonth = ori_data['Month']\n",
    "days_each_month = np.cumsum(np.array([0,31,28,31,30,31,30,31,31,30,31,30,31]))\n",
    "nday = [days_each_month[arg-1] for arg in nmonth.values]\n",
    "nday = nday + ori_data[\"Start_Time\"].dt.day.values\n",
    "ori_data['Day'] = nday\n",
    "\n",
    "ori_data['Weekday'] = ori_data['Start_Time'].dt.weekday\n",
    "# ori_data['Year'] = ori_data['Start_Time'].dt.year\n",
    "ori_data.drop('Start_Time', axis=1, inplace=True)\n",
    "ori_data.drop('End_Time', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary parentheses and 'city'\n",
    "ori_data['County'] = ori_data['County'].str.replace(r'\\(|\\)|city', '', case=False, regex=True)\n",
    "ori_data['County'] = ori_data['County'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data = ori_data.groupby('County').filter(lambda x: x['Temperature(F)'].notna().sum() >= 1 and x['Humidity(%)'].notna().sum() >= 1 and x['Visibility(mi)'].notna().sum() >= 1 and x['Wind_Speed(mph)'].notna().sum() >= 1 and x['Pressure(in)'].notna().sum() >= 1)\n",
    "\n",
    "ori_data.drop(ori_data[ori_data['Weather_Timestamp'].isna()].index, inplace=True)\n",
    "ori_data['Weather_Timestamp'] = pd.to_datetime(ori_data['Weather_Timestamp'])\n",
    "ori_data.sort_values('Weather_Timestamp', inplace=True)\n",
    "\n",
    "def fill_in_missing_value(missing_column: str, ori_data: pd.DataFrame):\n",
    "    # interpolate missing data and then fill in the rest(usaully start or end of the data)\n",
    "    ori_data[missing_column] = ori_data.groupby('County')[missing_column].transform(lambda x: x.interpolate(method='nearest').bfill().ffill())\n",
    "\n",
    "\n",
    "fill_in_missing_value('Temperature(F)', ori_data)\n",
    "fill_in_missing_value('Humidity(%)', ori_data)\n",
    "fill_in_missing_value('Visibility(mi)', ori_data)\n",
    "fill_in_missing_value('Wind_Speed(mph)', ori_data)\n",
    "fill_in_missing_value('Pressure(in)', ori_data)\n",
    "\n",
    "ori_data.drop('Weather_Timestamp', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'SW' 'CALM' 'W' 'S' 'NW' 'VAR' 'SE' 'E' 'NE']\n"
     ]
    }
   ],
   "source": [
    "wind_serie = ori_data['Wind_Direction']\n",
    "ori_data.drop(wind_serie[wind_serie.isnull()].index, inplace=True)\n",
    "ori_data['Wind_Direction'].replace(to_replace=['Calm'], value='CALM', inplace=True)\n",
    "ori_data['Wind_Direction'].replace(to_replace=['SSW', 'SSE', 'South'], value='S', inplace=True)\n",
    "ori_data['Wind_Direction'].replace(to_replace=['NNW', 'NNE', 'North'], value='N', inplace=True)\n",
    "ori_data['Wind_Direction'].replace(to_replace=['ESE', 'ENE', 'East'], value='E', inplace=True)\n",
    "ori_data['Wind_Direction'].replace(to_replace=['WSW', 'WNW', 'West'], value='W', inplace=True)\n",
    "ori_data['Wind_Direction'].replace(to_replace=['Variable'], value='VAR', inplace=True)\n",
    "print(ori_data['Wind_Direction'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data.drop(ori_data[ori_data['Weather_Condition'].isnull()].index, inplace=True)\n",
    "ori_data.drop(ori_data[ori_data['Weather_Condition'].str.contains('N/A', case=False)].index, inplace=True)\n",
    "ori_data.drop(ori_data[ori_data['Street'].isnull()].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data['Clear'] = np.where(ori_data['Weather_Condition'].str.contains('Clear|Fair', case=False, na=False), True, False)\n",
    "ori_data['Cloud'] = np.where(ori_data['Weather_Condition'].str.contains('Cloud|Overcast', case=False, na=False), True, False)\n",
    "ori_data['Rain'] = np.where(ori_data['Weather_Condition'].str.contains('Rain|Storm', case=False, na=False), True, False)\n",
    "ori_data['Heavy_Rain'] = np.where(ori_data['Weather_Condition'].str.contains('Heavy Rain|Thunderstorms|Heavy T-Storm', case=False, na=False), True, False)\n",
    "ori_data['Snow'] = np.where(ori_data['Weather_Condition'].str.contains('Snow|Hail|Sleet|Ice', case=False, na=False), True, False)\n",
    "ori_data['Heavy_Snow'] = np.where(ori_data['Weather_Condition'].str.contains('Heavy Snow|Heavy Ice|Heavy Sleet', case=False, na=False), True, False)\n",
    "ori_data['Fog'] = np.where(ori_data['Weather_Condition'].str.contains('Fog|Haze|Dust|Volcanic Ash|Smoke', case=False, na=False), True, False)\n",
    "ori_data['Windy'] = np.where(ori_data['Weather_Condition'].str.contains('Wind|Tornado', case=False, na=False), True, False)\n",
    "\n",
    "ori_data.drop('Weather_Condition', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 40 most common words\n",
      ", Rd, Dr, St, Ave, Ln, N, S, E, W, Blvd, Ct, Way, Highway, Pl, SW, Cir, Hwy, NE, NW, State, Road, Pkwy, SE, Old, Creek, US, Hill, Park, Lake, County, Trl, Route, Ter, Valley, Ridge, Mill, Oak, River, Loop\n"
     ]
    }
   ],
   "source": [
    "# https://www.responserack.com/nfirs/element/street-type-85/\n",
    "# create a list of top 40 most common words in street name\n",
    "st_type =' '.join(ori_data['Street'].unique().tolist()) # flat the array of street name\n",
    "st_type = re.split(\" |-\", st_type) # split the long string by space and hyphen\n",
    "st_type = [x[0] for x in Counter(st_type).most_common(40)] # select the 40 most common words\n",
    "print('the 40 most common words')\n",
    "print(*st_type, sep = \", \") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 40 most common words\n",
      ", Rd, St, Ave, S, N, W, E, Blvd, Highway, I, Hwy, US, Dr, Pkwy, State, Fwy, Expy, NW, CA, Pike, SW, NE, Route, Ln, Memorial, Lake, Old, SE, River, Road, Creek, Valley, Tpke, New, San, Canyon, Loop, Washington, Trl\n"
     ]
    }
   ],
   "source": [
    "top_streets=ori_data[\"Street\"].value_counts().sort_values()[-10000:].reset_index()\n",
    "st_type =' '.join(top_streets['Street'].unique().tolist()) # flat the array of street name\n",
    "st_type = re.split(\" |-\", st_type) # split the long string by space and hyphen\n",
    "st_type = [x[0] for x in Counter(st_type).most_common(40)] # select the 40 most common words\n",
    "print('the 40 most common words')\n",
    "print(*st_type, sep = \", \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_type = [' Rd', ' Dr', ' St', ' Ave', ' Ln', 'Blvd', ' Ct', ' Way', ' Highway', ' Pl', ' Cir', ' Hwy', ' Road', ' Pkwy', 'Old', 'Creek', 'Hill', 'Park', 'Lake', 'Trl', 'US-', ' Route', 'I-', ' Fwy', ' Expy', 'Tpke']\n",
    "\n",
    "def find_street_type(x):\n",
    "    for st in st_type:\n",
    "        if x.find(st) != -1:\n",
    "            return st.strip()\n",
    "    return 'Other'\n",
    "ori_data['street_type'] = ori_data['Street'].apply(find_street_type)\n",
    "ori_data.drop('Street', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Feature, Missing_Percent(%)]\n",
      "Index: []\n",
      "(7481003, 41)\n",
      "Index(['Severity', 'Twilight', 'Start_Lat', 'Start_Lng', 'Distance(mi)',\n",
      "       'City', 'County', 'State', 'Temperature(F)', 'Humidity(%)',\n",
      "       'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)',\n",
      "       'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit',\n",
      "       'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming',\n",
      "       'Traffic_Signal', 'elapsed_time', 'Hour', 'Minute', 'Month', 'Day',\n",
      "       'Weekday', 'Clear', 'Cloud', 'Rain', 'Heavy_Rain', 'Snow', 'Heavy_Snow',\n",
      "       'Fog', 'Windy', 'street_type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "show_missing_rate(ori_data)\n",
    "print(ori_data.shape)\n",
    "print(ori_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severity 4\n",
      "Twilight 2\n",
      "Start_Lat 2359683\n",
      "Start_Lng 2409346\n",
      "Distance(mi) 21881\n",
      "City 12266\n",
      "County 1746\n",
      "State 49\n",
      "Temperature(F) 859\n",
      "Humidity(%) 100\n",
      "Pressure(in) 1137\n",
      "Visibility(mi) 91\n",
      "Wind_Direction 10\n",
      "Wind_Speed(mph) 184\n",
      "Amenity 2\n",
      "Bump 2\n",
      "Crossing 2\n",
      "Give_Way 2\n",
      "Junction 2\n",
      "No_Exit 2\n",
      "Railway 2\n",
      "Roundabout 2\n",
      "Station 2\n",
      "Stop 2\n",
      "Traffic_Calming 2\n",
      "Traffic_Signal 2\n",
      "elapsed_time 73249\n",
      "Hour 24\n",
      "Minute 1440\n",
      "Month 12\n",
      "Day 365\n",
      "Weekday 7\n",
      "Clear 2\n",
      "Cloud 2\n",
      "Rain 2\n",
      "Heavy_Rain 2\n",
      "Snow 2\n",
      "Heavy_Snow 2\n",
      "Fog 2\n",
      "Windy 2\n",
      "street_type 27\n"
     ]
    }
   ],
   "source": [
    "for col in ori_data.columns:\n",
    "    print(col, ori_data[col].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Twilight', 'City', 'County', 'State', 'Wind_Direction', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Month', 'Weekday', 'Clear', 'Cloud', 'Rain', 'Heavy_Rain', 'Snow', 'Heavy_Snow', 'Fog', 'Windy', 'street_type']\n"
     ]
    }
   ],
   "source": [
    "numerical_features = [\n",
    "    'Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)',\n",
    "    'Wind_Speed(mph)', 'elapsed_time', 'Start_Lat', 'Start_Lng', 'Hour', 'Day', 'Minute'\n",
    "]\n",
    "\n",
    "# categorical_features = [f for f in list(ori_data.columns) if f not in numerical_features]\n",
    "categorical_features = [f for f in list(ori_data.columns) if (f not in numerical_features and f != 'Severity')]\n",
    "print(categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "for column in categorical_features:\n",
    "    ori_data[column] = label_encoder.fit_transform(ori_data[column])\n",
    "    ori_data[column] = ori_data[column].astype(np.int32)\n",
    "\n",
    "# process continous value to float32\n",
    "for column in numerical_features:\n",
    "    ori_data[column] = ori_data[column].astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data.to_csv('./data/v2-4.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-accident-predict-4yLnMnDS-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
