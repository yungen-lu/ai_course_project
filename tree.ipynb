{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import classification_report\n",
            "from collections import Counter\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from imblearn.under_sampling import RandomUnderSampler\n",
            "import matplotlib.pyplot as plt\n",
            "rus = RandomUnderSampler(random_state=42)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ori_data = pd.read_csv('./data/v2-4.csv', index_col=False)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "numerical_features = [\n",
            "    'Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)',\n",
            "    'Wind_Speed(mph)', 'elapsed_time', 'Start_Lat', 'Start_Lng'\n",
            "]\n",
            "\n",
            "categorical_features = [f for f in list(ori_data.columns) if (f not in numerical_features)]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X = ori_data.drop(['Severity'], axis=1)\n",
            "y = ori_data['Severity']"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Self-define score function"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def custom_scoring(y_true, y_pred, verbose=False, equal_weighted_f1=False):\n",
            "    report = classification_report(y_true, y_pred, output_dict=True)\n",
            "    weighted_beta_f1 = 0\n",
            "    beta_weights = {\n",
            "        '1': 0.5,\n",
            "        '2': 1,\n",
            "        '3': 1,\n",
            "        '4': 2,\n",
            "    }\n",
            "    total_data_count = report['weighted avg']['support']\n",
            "    for cl in range(1, 5):\n",
            "        pr = report[str(cl)]['precision']\n",
            "        rc = report[str(cl)]['recall']\n",
            "        beta = beta_weights[str(cl)]\n",
            "        beta_f1 = ((1+beta**2)*pr*rc)/(pr*(beta**2) + rc)\n",
            "        if verbose: \n",
            "            print(f'beta f1 for level [{cl}]: {beta_f1}, pr: {pr}, rc: {rc}')\n",
            "\n",
            "        if not equal_weighted_f1:\n",
            "            support_proportion = report[str(cl)]['support'] / total_data_count\n",
            "            weighted_beta_f1 += beta_f1 * support_proportion\n",
            "        else:\n",
            "            weighted_beta_f1 += beta_f1*0.25\n",
            "\n",
            "    if verbose and equal_weighted_f1:\n",
            "        print(f\"macro avg for f1: {weighted_beta_f1}\")\n",
            "    return weighted_beta_f1"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### K-fold validation, evaluation function"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.model_selection import KFold\n",
            "from sklearn.metrics import precision_recall_curve, auc\n",
            "\n",
            "def cross_valid(X, y, estimator, cv=5, verbose=False, balance_cls=False, equal_weighted_f1=False):\n",
            "    total_f1 = 0\n",
            "    X.reset_index()\n",
            "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
            "    print('Validation data')\n",
            "    for i, (train_index, valid_index) in enumerate(kf.split(X)):\n",
            "        x_train, x_valid = X.iloc[train_index], X.iloc[valid_index]\n",
            "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
            "        \n",
            "        if balance_cls:\n",
            "            x_train, y_train = rus.fit_resample(x_train, y_train)\n",
            "            if verbose:\n",
            "                print('After under sampling:')\n",
            "                print(f'Length of training data: {len(x_train)}, and its distribution among each severity {Counter(y_train)}')\n",
            "\n",
            "        estimator.fit(x_train, y_train)\n",
            "        y_valid_pred = estimator.predict(x_valid)\n",
            "        beta_f1 = custom_scoring(y_valid, y_valid_pred, verbose=False, equal_weighted_f1=equal_weighted_f1)\n",
            "        print(f'Round {i} beta_f1: {beta_f1}')\n",
            "        total_f1 += beta_f1\n",
            "        \n",
            "    avg_betaf1 = total_f1 / cv\n",
            "    print(f'average beta f1-score after kfold: {avg_betaf1}')\n",
            "\n",
            "\n",
            "def test(estimator, x_test, y_test):\n",
            "    print('Testing data:')\n",
            "    y_test_pred = estimator.predict(x_test)\n",
            "    print(classification_report(y_test, y_test_pred))\n",
            "    beta_f1 = custom_scoring(y_test, y_test_pred, verbose=True)\n",
            "    print(f'beta f1-score: {beta_f1}')\n",
            "        \n",
            "def auc_pr(estimator, x_test, y_test):\n",
            "    y_scores = estimator.predict_proba(x_test)[:, 1]\n",
            "    precision = dict()\n",
            "    recall = dict()\n",
            "    n_classes = 4\n",
            "    for i in range(n_classes):\n",
            "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_scores[:, i])\n",
            "        plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n",
            "        \n",
            "    plt.xlabel(\"recall\")\n",
            "    plt.ylabel(\"precision\")\n",
            "    plt.legend(loc=\"best\")\n",
            "    plt.title(\"precision vs. recall curve\")\n",
            "    plt.show()\n",
            "        "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.multiclass import OneVsRestClassifier\n",
            "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.svm import LinearSVC"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "x_train_valid, x_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Direct Testing"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Decision Tree"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dt = DecisionTreeClassifier()\n",
            "dt.fit(x_train_valid, y_train_valid)\n",
            "test(dt, x_test, y_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Random Forest"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rfc = RandomForestClassifier(n_estimators=50, max_depth=15)\n",
            "rfc.fit(x_train_valid, y_train_valid)\n",
            "test(rfc, x_test, y_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### OneVsRestClassifier with decision tree"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = OneVsRestClassifier(DecisionTreeClassifier(max_depth=5)).fit(x_train_valid, y_train_valid)\n",
            "test(clf, x_test, y_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### OneVsRestClassifier with RandomForest"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = OneVsRestClassifier(RandomForestClassifier(max_depth=15)).fit(x_train_valid, y_train_valid)\n",
            "test(clf, x_test, y_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## **KFold cross validation**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Decision Tree"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Decision tree without balance dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dt = DecisionTreeClassifier()\n",
            "cross_valid(x_train_valid, y_train_valid, dt, verbose=True, balance_cls=False, equal_weighted_f1=True)\n",
            "test(dt, x_test, y_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Decision tree with balance dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dt = DecisionTreeClassifier()\n",
            "cross_valid(x_train_valid, y_train_valid, dt, verbose=True, balance_cls=True, equal_weighted_f1=True)\n",
            "test(dt, x_test, y_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### OneVsRestClassifier with decision tree as base model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "without balance dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = OneVsRestClassifier(DecisionTreeClassifier())\n",
            "cross_valid(x_train_valid, y_train_valid, clf, verbose=True, balance_cls=False, equal_weighted_f1=True)\n",
            "test(clf, x_test, y_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "with balance dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = OneVsRestClassifier(DecisionTreeClassifier())\n",
            "cross_valid(x_train_valid, y_train_valid, clf, verbose=True, balance_cls=True, equal_weighted_f1=True)\n",
            "test(clf, x_test, y_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### OneVsRestClassifier with Random Forest as base model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Without balance"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = OneVsRestClassifier(RandomForestClassifier())\n",
            "cross_valid(x_train_valid, y_train_valid, clf, verbose=True, balance_cls=False, equal_weighted_f1=True)\n",
            "test(clf, x_test, y_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Balance"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = OneVsRestClassifier(RandomForestClassifier())\n",
            "cross_valid(x_train_valid, y_train_valid, clf, verbose=True, balance_cls=True, equal_weighted_f1=True)\n",
            "test(clf, x_test, y_test)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.6"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
